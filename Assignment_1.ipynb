{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrpcjqiuSvCJ",
        "outputId": "db9f33ef-d769-4354-94ff-a2ec084fc9f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensorflow Version: 2.9.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print('Tensorflow Version:', tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAOveIZyTGIR",
        "outputId": "1b7989c4-8a4f-4d7b-da8a-eab0ee094d79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello Tensorflow!\n"
          ]
        }
      ],
      "source": [
        "msg = tf.constant('Hello Tensorflow!')\n",
        "tf.print(msg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oce9g6goTSRx",
        "outputId": "59663fa7-0740-4329-a3df-769961935652"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keras Version: 2.9.0\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "print('Keras Version:', keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7adMJ0KaU4rE",
        "outputId": "83aad625-7cbf-4d62-bd19-27cac7aa6436"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 6s 11ms/step - loss: 0.2545 - accuracy: 0.9252\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.1036 - accuracy: 0.9690\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 7s 14ms/step - loss: 0.0689 - accuracy: 0.9790\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.0501 - accuracy: 0.9856\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 7s 15ms/step - loss: 0.0382 - accuracy: 0.9882\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0675 - accuracy: 0.9788\n",
            "Testing :  0.9787999987602234\n",
            "Training :  0.06750303506851196\n"
          ]
        }
      ],
      "source": [
        "from keras import datasets\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Load MNIST data\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
        "\n",
        "# Creating network consists of two dense layers(fully connected layer)\n",
        "network = models.Sequential()\n",
        "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,), name=\"layer1\"))\n",
        "network.add(layers.Dense(10, activation='softmax', name=\"layer2\"))\n",
        "\n",
        "# Preparing the training images and labels\n",
        "train_images = train_images.reshape(60000, 28*28)\n",
        "train_images = train_images.astype('float32') / 255\n",
        "train_labels = to_categorical(train_labels)\n",
        "\n",
        "# Preparing the testing images and labels\n",
        "test_images = test_images.reshape(10000, 28*28)\n",
        "test_images = test_images.astype('float32') / 255\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "# Prepare the network --> rmsprop : root mean square gradient descent\n",
        "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit the neural network\n",
        "network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
        "\n",
        "# Evaluate the network performance\n",
        "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
        "\n",
        "# Print testing accuracy and testing loss\n",
        "print('Testing : ', test_acc)\n",
        "print('Training : ', test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch as t\n",
        "import torchvision.datasets as datasets \n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "# Create Tensors to hold input and outputs.\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
        "# we can consider it as a linear layer neural network. Let's prepare the\n",
        "# tensor (x, x^2, x^3).\n",
        "p = torch.tensor([1, 2, 3])\n",
        "xx = x.unsqueeze(-1).pow(p)\n",
        "\n",
        "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
        "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
        "# of shape (2000, 3) \n",
        "\n",
        "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
        "# is a Module which contains other Modules, and applies them in sequence to\n",
        "# produce its output. The Linear Module computes output from input using a\n",
        "# linear function, and holds internal Tensors for its weight and bias.\n",
        "# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n",
        "# to match the shape of `y`.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(3, 1),\n",
        "    torch.nn.Flatten(0, 1)\n",
        ")\n",
        "\n",
        "# The nn package also contains definitions of popular loss functions; in this\n",
        "# case we will use Mean Squared Error (MSE) as our loss function.\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "\n",
        "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
        "    # override the __call__ operator so you can call them like functions. When\n",
        "    # doing so you pass a Tensor of input data to the Module and it produces\n",
        "    # a Tensor of output data.\n",
        "    y_pred = model(xx)\n",
        "\n",
        "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
        "    # values of y, and the loss function returns a Tensor containing the\n",
        "    # loss.\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Zero the gradients before running the backward pass.\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
        "    # parameters of the model. Internally, the parameters of each Module are stored\n",
        "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
        "    # all learnable parameters in the model.\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
        "    # we can access its gradients like we did before.\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= learning_rate * param.grad\n",
        "\n",
        "# You can access the first layer of `model` like accessing the first item of a list\n",
        "linear_layer = model[0]\n",
        "\n",
        "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
        "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
